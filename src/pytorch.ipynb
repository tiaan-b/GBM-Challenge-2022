{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download training data\n",
    "train_data = datasets.FashionMNIST(\n",
    "    root='./fashiondata',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "#  download test data\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root='./fashiondata',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X [N, C, H, W]: torch.Size([6, 1, 28, 28])\n",
      "Shape of y: torch.Size([6]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "batch_size = 6\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "for X, y in train_loader:\n",
    "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for training\n",
      "Device: NVIDIA GeForce GTX 980 Ti\n",
      "Memory Usage: 0.011 GB\n",
      "Cuda version: 11.6\n",
      "Cudnn version: 8302\n"
     ]
    }
   ],
   "source": [
    "# Get cpu or gpu device for training \n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} for training\")\n",
    "\n",
    "# output the device info\n",
    "if device == \"cuda\":\n",
    "    print(f\"Device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory Usage: {torch.cuda.memory_allocated(0) / 1024**3:.3f} GB\")\n",
    "    print(f\"Cuda version: {torch.version.cuda}\")\n",
    "    print(f\"Cudnn version: {torch.backends.cudnn.version()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        pred = model.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f} [{current:>5d}/{size:<5d}]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(dim=1) == y).type(torch.float).sum().item()\n",
    "        test_loss /= num_batches\n",
    "        correct /= size\n",
    "        print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "--------------------------------\n",
      "loss: 1.244875 [    0/60000]\n",
      "loss: 0.912305 [  600/60000]\n",
      "loss: 1.633523 [ 1200/60000]\n",
      "loss: 0.424719 [ 1800/60000]\n",
      "loss: 2.170732 [ 2400/60000]\n",
      "loss: 1.218477 [ 3000/60000]\n",
      "loss: 1.185403 [ 3600/60000]\n",
      "loss: 0.129139 [ 4200/60000]\n",
      "loss: 0.721709 [ 4800/60000]\n",
      "loss: 0.759470 [ 5400/60000]\n",
      "loss: 0.687891 [ 6000/60000]\n",
      "loss: 0.624871 [ 6600/60000]\n",
      "loss: 0.853191 [ 7200/60000]\n",
      "loss: 0.765871 [ 7800/60000]\n",
      "loss: 0.181301 [ 8400/60000]\n",
      "loss: 0.220630 [ 9000/60000]\n",
      "loss: 0.789909 [ 9600/60000]\n",
      "loss: 0.496688 [10200/60000]\n",
      "loss: 0.503745 [10800/60000]\n",
      "loss: 0.271242 [11400/60000]\n",
      "loss: 0.567298 [12000/60000]\n",
      "loss: 0.618639 [12600/60000]\n",
      "loss: 0.886782 [13200/60000]\n",
      "loss: 0.447342 [13800/60000]\n",
      "loss: 0.142310 [14400/60000]\n",
      "loss: 0.667118 [15000/60000]\n",
      "loss: 0.064065 [15600/60000]\n",
      "loss: 0.102207 [16200/60000]\n",
      "loss: 1.028983 [16800/60000]\n",
      "loss: 0.146481 [17400/60000]\n",
      "loss: 1.200664 [18000/60000]\n",
      "loss: 0.104033 [18600/60000]\n",
      "loss: 0.344884 [19200/60000]\n",
      "loss: 0.536741 [19800/60000]\n",
      "loss: 0.477688 [20400/60000]\n",
      "loss: 0.091037 [21000/60000]\n",
      "loss: 0.792947 [21600/60000]\n",
      "loss: 0.030645 [22200/60000]\n",
      "loss: 0.388239 [22800/60000]\n",
      "loss: 0.664383 [23400/60000]\n",
      "loss: 1.000811 [24000/60000]\n",
      "loss: 0.055566 [24600/60000]\n",
      "loss: 0.114329 [25200/60000]\n",
      "loss: 0.538500 [25800/60000]\n",
      "loss: 0.121830 [26400/60000]\n",
      "loss: 0.021019 [27000/60000]\n",
      "loss: 1.009748 [27600/60000]\n",
      "loss: 0.635004 [28200/60000]\n",
      "loss: 0.112135 [28800/60000]\n",
      "loss: 0.113312 [29400/60000]\n",
      "loss: 0.097110 [30000/60000]\n",
      "loss: 0.015654 [30600/60000]\n",
      "loss: 0.618183 [31200/60000]\n",
      "loss: 0.338480 [31800/60000]\n",
      "loss: 0.772782 [32400/60000]\n",
      "loss: 0.860142 [33000/60000]\n",
      "loss: 0.419642 [33600/60000]\n",
      "loss: 0.222347 [34200/60000]\n",
      "loss: 0.905423 [34800/60000]\n",
      "loss: 0.181446 [35400/60000]\n",
      "loss: 0.011519 [36000/60000]\n",
      "loss: 0.132374 [36600/60000]\n",
      "loss: 0.144733 [37200/60000]\n",
      "loss: 0.506708 [37800/60000]\n",
      "loss: 0.551671 [38400/60000]\n",
      "loss: 1.053995 [39000/60000]\n",
      "loss: 0.137260 [39600/60000]\n",
      "loss: 0.150755 [40200/60000]\n",
      "loss: 0.274079 [40800/60000]\n",
      "loss: 0.211706 [41400/60000]\n",
      "loss: 0.048561 [42000/60000]\n",
      "loss: 0.968650 [42600/60000]\n",
      "loss: 0.011443 [43200/60000]\n",
      "loss: 0.308150 [43800/60000]\n",
      "loss: 0.448631 [44400/60000]\n",
      "loss: 0.701435 [45000/60000]\n",
      "loss: 0.290174 [45600/60000]\n",
      "loss: 0.341767 [46200/60000]\n",
      "loss: 0.093515 [46800/60000]\n",
      "loss: 0.201490 [47400/60000]\n",
      "loss: 0.126418 [48000/60000]\n",
      "loss: 0.224648 [48600/60000]\n",
      "loss: 0.270773 [49200/60000]\n",
      "loss: 0.134685 [49800/60000]\n",
      "loss: 0.516588 [50400/60000]\n",
      "loss: 0.907176 [51000/60000]\n",
      "loss: 1.439271 [51600/60000]\n",
      "loss: 0.371906 [52200/60000]\n",
      "loss: 0.383627 [52800/60000]\n",
      "loss: 0.380407 [53400/60000]\n",
      "loss: 0.229316 [54000/60000]\n",
      "loss: 0.343476 [54600/60000]\n",
      "loss: 0.119523 [55200/60000]\n",
      "loss: 0.792265 [55800/60000]\n",
      "loss: 0.725445 [56400/60000]\n",
      "loss: 0.679785 [57000/60000]\n",
      "loss: 1.177987 [57600/60000]\n",
      "loss: 0.095299 [58200/60000]\n",
      "loss: 0.576614 [58800/60000]\n",
      "loss: 0.380676 [59400/60000]\n",
      "Test Error: \n",
      " Accuracy: 82.5%, Avg loss: 0.481853 \n",
      "\n",
      "Epoch 2\n",
      "--------------------------------\n",
      "loss: 1.020522 [    0/60000]\n",
      "loss: 0.372679 [  600/60000]\n",
      "loss: 0.839151 [ 1200/60000]\n",
      "loss: 0.248299 [ 1800/60000]\n",
      "loss: 0.781013 [ 2400/60000]\n",
      "loss: 0.411553 [ 3000/60000]\n",
      "loss: 0.325933 [ 3600/60000]\n",
      "loss: 0.051198 [ 4200/60000]\n",
      "loss: 0.359682 [ 4800/60000]\n",
      "loss: 0.563135 [ 5400/60000]\n",
      "loss: 1.015086 [ 6000/60000]\n",
      "loss: 0.277667 [ 6600/60000]\n",
      "loss: 1.241643 [ 7200/60000]\n",
      "loss: 0.535861 [ 7800/60000]\n",
      "loss: 0.029790 [ 8400/60000]\n",
      "loss: 0.032207 [ 9000/60000]\n",
      "loss: 0.579069 [ 9600/60000]\n",
      "loss: 0.217525 [10200/60000]\n",
      "loss: 0.786412 [10800/60000]\n",
      "loss: 0.302605 [11400/60000]\n",
      "loss: 0.292261 [12000/60000]\n",
      "loss: 0.608934 [12600/60000]\n",
      "loss: 0.848154 [13200/60000]\n",
      "loss: 0.227395 [13800/60000]\n",
      "loss: 0.060156 [14400/60000]\n",
      "loss: 0.248708 [15000/60000]\n",
      "loss: 0.023568 [15600/60000]\n",
      "loss: 0.074270 [16200/60000]\n",
      "loss: 0.760295 [16800/60000]\n",
      "loss: 0.029171 [17400/60000]\n",
      "loss: 0.671743 [18000/60000]\n",
      "loss: 0.089500 [18600/60000]\n",
      "loss: 0.027319 [19200/60000]\n",
      "loss: 0.493014 [19800/60000]\n",
      "loss: 0.547964 [20400/60000]\n",
      "loss: 0.083127 [21000/60000]\n",
      "loss: 0.636658 [21600/60000]\n",
      "loss: 0.014656 [22200/60000]\n",
      "loss: 0.156916 [22800/60000]\n",
      "loss: 0.313089 [23400/60000]\n",
      "loss: 0.819260 [24000/60000]\n",
      "loss: 0.058155 [24600/60000]\n",
      "loss: 0.061054 [25200/60000]\n",
      "loss: 0.506521 [25800/60000]\n",
      "loss: 0.117924 [26400/60000]\n",
      "loss: 0.006921 [27000/60000]\n",
      "loss: 0.626492 [27600/60000]\n",
      "loss: 0.396070 [28200/60000]\n",
      "loss: 0.040081 [28800/60000]\n",
      "loss: 0.018640 [29400/60000]\n",
      "loss: 0.033071 [30000/60000]\n",
      "loss: 0.028912 [30600/60000]\n",
      "loss: 0.432687 [31200/60000]\n",
      "loss: 0.058571 [31800/60000]\n",
      "loss: 0.942007 [32400/60000]\n",
      "loss: 0.655759 [33000/60000]\n",
      "loss: 0.506855 [33600/60000]\n",
      "loss: 0.290278 [34200/60000]\n",
      "loss: 0.733824 [34800/60000]\n",
      "loss: 0.096873 [35400/60000]\n",
      "loss: 0.025673 [36000/60000]\n",
      "loss: 0.091162 [36600/60000]\n",
      "loss: 0.060474 [37200/60000]\n",
      "loss: 0.281875 [37800/60000]\n",
      "loss: 0.145048 [38400/60000]\n",
      "loss: 1.238116 [39000/60000]\n",
      "loss: 0.254063 [39600/60000]\n",
      "loss: 0.199725 [40200/60000]\n",
      "loss: 0.297529 [40800/60000]\n",
      "loss: 0.109856 [41400/60000]\n",
      "loss: 0.078723 [42000/60000]\n",
      "loss: 0.776202 [42600/60000]\n",
      "loss: 0.003200 [43200/60000]\n",
      "loss: 0.164009 [43800/60000]\n",
      "loss: 0.535525 [44400/60000]\n",
      "loss: 0.644091 [45000/60000]\n",
      "loss: 0.267135 [45600/60000]\n",
      "loss: 0.269909 [46200/60000]\n",
      "loss: 0.164718 [46800/60000]\n",
      "loss: 0.182994 [47400/60000]\n",
      "loss: 0.136851 [48000/60000]\n",
      "loss: 0.073223 [48600/60000]\n",
      "loss: 0.153234 [49200/60000]\n",
      "loss: 0.104831 [49800/60000]\n",
      "loss: 0.418913 [50400/60000]\n",
      "loss: 0.635045 [51000/60000]\n",
      "loss: 1.404556 [51600/60000]\n",
      "loss: 0.208559 [52200/60000]\n",
      "loss: 0.295668 [52800/60000]\n",
      "loss: 0.367633 [53400/60000]\n",
      "loss: 0.208305 [54000/60000]\n",
      "loss: 0.179786 [54600/60000]\n",
      "loss: 0.106680 [55200/60000]\n",
      "loss: 0.468871 [55800/60000]\n",
      "loss: 0.314423 [56400/60000]\n",
      "loss: 1.101678 [57000/60000]\n",
      "loss: 1.043802 [57600/60000]\n",
      "loss: 0.368253 [58200/60000]\n",
      "loss: 0.531297 [58800/60000]\n",
      "loss: 0.245882 [59400/60000]\n",
      "Test Error: \n",
      " Accuracy: 84.9%, Avg loss: 0.409234 \n",
      "\n",
      "Epoch 3\n",
      "--------------------------------\n",
      "loss: 0.822900 [    0/60000]\n",
      "loss: 0.421761 [  600/60000]\n",
      "loss: 0.646254 [ 1200/60000]\n",
      "loss: 0.150664 [ 1800/60000]\n",
      "loss: 0.523866 [ 2400/60000]\n",
      "loss: 0.418077 [ 3000/60000]\n",
      "loss: 0.375219 [ 3600/60000]\n",
      "loss: 0.061962 [ 4200/60000]\n",
      "loss: 0.934726 [ 4800/60000]\n",
      "loss: 0.594313 [ 5400/60000]\n",
      "loss: 0.969419 [ 6000/60000]\n",
      "loss: 0.331595 [ 6600/60000]\n",
      "loss: 0.973509 [ 7200/60000]\n",
      "loss: 0.492350 [ 7800/60000]\n",
      "loss: 0.031090 [ 8400/60000]\n",
      "loss: 0.055768 [ 9000/60000]\n",
      "loss: 0.523940 [ 9600/60000]\n",
      "loss: 0.133423 [10200/60000]\n",
      "loss: 0.291722 [10800/60000]\n",
      "loss: 0.333060 [11400/60000]\n",
      "loss: 0.245795 [12000/60000]\n",
      "loss: 0.358274 [12600/60000]\n",
      "loss: 0.966984 [13200/60000]\n",
      "loss: 0.129499 [13800/60000]\n",
      "loss: 0.064589 [14400/60000]\n",
      "loss: 0.084859 [15000/60000]\n",
      "loss: 0.017262 [15600/60000]\n",
      "loss: 0.044666 [16200/60000]\n",
      "loss: 0.849202 [16800/60000]\n",
      "loss: 0.036872 [17400/60000]\n",
      "loss: 0.561658 [18000/60000]\n",
      "loss: 0.032601 [18600/60000]\n",
      "loss: 0.029919 [19200/60000]\n",
      "loss: 0.384702 [19800/60000]\n",
      "loss: 0.675363 [20400/60000]\n",
      "loss: 0.075141 [21000/60000]\n",
      "loss: 0.607489 [21600/60000]\n",
      "loss: 0.007165 [22200/60000]\n",
      "loss: 0.104634 [22800/60000]\n",
      "loss: 0.558540 [23400/60000]\n",
      "loss: 0.938865 [24000/60000]\n",
      "loss: 0.095853 [24600/60000]\n",
      "loss: 0.052562 [25200/60000]\n",
      "loss: 0.597611 [25800/60000]\n",
      "loss: 0.070258 [26400/60000]\n",
      "loss: 0.003944 [27000/60000]\n",
      "loss: 0.271829 [27600/60000]\n",
      "loss: 0.277289 [28200/60000]\n",
      "loss: 0.058131 [28800/60000]\n",
      "loss: 0.010940 [29400/60000]\n",
      "loss: 0.015959 [30000/60000]\n",
      "loss: 0.019634 [30600/60000]\n",
      "loss: 0.541714 [31200/60000]\n",
      "loss: 0.016183 [31800/60000]\n",
      "loss: 0.513990 [32400/60000]\n",
      "loss: 0.518039 [33000/60000]\n",
      "loss: 0.498620 [33600/60000]\n",
      "loss: 0.349570 [34200/60000]\n",
      "loss: 0.760202 [34800/60000]\n",
      "loss: 0.073175 [35400/60000]\n",
      "loss: 0.070387 [36000/60000]\n",
      "loss: 0.079637 [36600/60000]\n",
      "loss: 0.022231 [37200/60000]\n",
      "loss: 0.272642 [37800/60000]\n",
      "loss: 0.113441 [38400/60000]\n",
      "loss: 0.675646 [39000/60000]\n",
      "loss: 0.154270 [39600/60000]\n",
      "loss: 0.202112 [40200/60000]\n",
      "loss: 0.320975 [40800/60000]\n",
      "loss: 0.191269 [41400/60000]\n",
      "loss: 0.059019 [42000/60000]\n",
      "loss: 0.718729 [42600/60000]\n",
      "loss: 0.009689 [43200/60000]\n",
      "loss: 0.196530 [43800/60000]\n",
      "loss: 0.338360 [44400/60000]\n",
      "loss: 0.573301 [45000/60000]\n",
      "loss: 0.193265 [45600/60000]\n",
      "loss: 0.231208 [46200/60000]\n",
      "loss: 0.149644 [46800/60000]\n",
      "loss: 0.271487 [47400/60000]\n",
      "loss: 0.176440 [48000/60000]\n",
      "loss: 0.039823 [48600/60000]\n",
      "loss: 0.134585 [49200/60000]\n",
      "loss: 0.118295 [49800/60000]\n",
      "loss: 0.434760 [50400/60000]\n",
      "loss: 0.709617 [51000/60000]\n",
      "loss: 2.469637 [51600/60000]\n",
      "loss: 0.215351 [52200/60000]\n",
      "loss: 0.237984 [52800/60000]\n",
      "loss: 0.261192 [53400/60000]\n",
      "loss: 0.144339 [54000/60000]\n",
      "loss: 0.114019 [54600/60000]\n",
      "loss: 0.073774 [55200/60000]\n",
      "loss: 0.425946 [55800/60000]\n",
      "loss: 0.258786 [56400/60000]\n",
      "loss: 0.504506 [57000/60000]\n",
      "loss: 0.531788 [57600/60000]\n",
      "loss: 0.630636 [58200/60000]\n",
      "loss: 0.437785 [58800/60000]\n",
      "loss: 0.156220 [59400/60000]\n",
      "Test Error: \n",
      " Accuracy: 86.1%, Avg loss: 0.392694 \n",
      "\n",
      "Epoch 4\n",
      "--------------------------------\n",
      "loss: 0.581640 [    0/60000]\n",
      "loss: 0.148208 [  600/60000]\n",
      "loss: 1.039577 [ 1200/60000]\n",
      "loss: 0.124062 [ 1800/60000]\n",
      "loss: 0.558873 [ 2400/60000]\n",
      "loss: 0.337488 [ 3000/60000]\n",
      "loss: 0.543365 [ 3600/60000]\n",
      "loss: 0.213711 [ 4200/60000]\n",
      "loss: 0.817346 [ 4800/60000]\n",
      "loss: 0.527813 [ 5400/60000]\n",
      "loss: 0.967859 [ 6000/60000]\n",
      "loss: 0.192656 [ 6600/60000]\n",
      "loss: 0.770595 [ 7200/60000]\n",
      "loss: 0.590518 [ 7800/60000]\n",
      "loss: 0.034252 [ 8400/60000]\n",
      "loss: 0.035128 [ 9000/60000]\n",
      "loss: 0.418415 [ 9600/60000]\n",
      "loss: 0.122540 [10200/60000]\n",
      "loss: 0.363445 [10800/60000]\n",
      "loss: 0.333874 [11400/60000]\n",
      "loss: 0.312105 [12000/60000]\n",
      "loss: 0.238828 [12600/60000]\n",
      "loss: 0.575720 [13200/60000]\n",
      "loss: 0.169416 [13800/60000]\n",
      "loss: 0.056387 [14400/60000]\n",
      "loss: 0.131686 [15000/60000]\n",
      "loss: 0.020986 [15600/60000]\n",
      "loss: 0.029860 [16200/60000]\n",
      "loss: 1.035375 [16800/60000]\n",
      "loss: 0.015002 [17400/60000]\n",
      "loss: 0.523986 [18000/60000]\n",
      "loss: 0.044343 [18600/60000]\n",
      "loss: 0.014523 [19200/60000]\n",
      "loss: 0.434587 [19800/60000]\n",
      "loss: 0.547516 [20400/60000]\n",
      "loss: 0.093728 [21000/60000]\n",
      "loss: 0.622853 [21600/60000]\n",
      "loss: 0.004528 [22200/60000]\n",
      "loss: 0.104074 [22800/60000]\n",
      "loss: 0.567634 [23400/60000]\n",
      "loss: 0.886696 [24000/60000]\n",
      "loss: 0.084021 [24600/60000]\n",
      "loss: 0.035349 [25200/60000]\n",
      "loss: 0.425203 [25800/60000]\n",
      "loss: 0.059977 [26400/60000]\n",
      "loss: 0.003894 [27000/60000]\n",
      "loss: 0.316235 [27600/60000]\n",
      "loss: 0.224006 [28200/60000]\n",
      "loss: 0.028858 [28800/60000]\n",
      "loss: 0.010424 [29400/60000]\n",
      "loss: 0.026903 [30000/60000]\n",
      "loss: 0.024247 [30600/60000]\n",
      "loss: 0.430210 [31200/60000]\n",
      "loss: 0.019798 [31800/60000]\n",
      "loss: 0.818655 [32400/60000]\n",
      "loss: 0.716788 [33000/60000]\n",
      "loss: 0.411018 [33600/60000]\n",
      "loss: 0.310923 [34200/60000]\n",
      "loss: 0.862577 [34800/60000]\n",
      "loss: 0.098104 [35400/60000]\n",
      "loss: 0.078372 [36000/60000]\n",
      "loss: 0.112881 [36600/60000]\n",
      "loss: 0.037925 [37200/60000]\n",
      "loss: 0.240913 [37800/60000]\n",
      "loss: 0.022518 [38400/60000]\n",
      "loss: 0.415427 [39000/60000]\n",
      "loss: 0.321705 [39600/60000]\n",
      "loss: 0.388285 [40200/60000]\n",
      "loss: 0.255183 [40800/60000]\n",
      "loss: 0.167002 [41400/60000]\n",
      "loss: 0.065701 [42000/60000]\n",
      "loss: 0.681871 [42600/60000]\n",
      "loss: 0.001791 [43200/60000]\n",
      "loss: 0.123826 [43800/60000]\n",
      "loss: 0.300037 [44400/60000]\n",
      "loss: 0.604750 [45000/60000]\n",
      "loss: 0.128934 [45600/60000]\n",
      "loss: 0.332670 [46200/60000]\n",
      "loss: 0.193530 [46800/60000]\n",
      "loss: 0.341175 [47400/60000]\n",
      "loss: 0.142530 [48000/60000]\n",
      "loss: 0.011791 [48600/60000]\n",
      "loss: 0.189026 [49200/60000]\n",
      "loss: 0.031974 [49800/60000]\n",
      "loss: 0.341754 [50400/60000]\n",
      "loss: 0.736564 [51000/60000]\n",
      "loss: 2.010210 [51600/60000]\n",
      "loss: 0.332996 [52200/60000]\n",
      "loss: 0.335543 [52800/60000]\n",
      "loss: 0.275560 [53400/60000]\n",
      "loss: 0.138809 [54000/60000]\n",
      "loss: 0.096697 [54600/60000]\n",
      "loss: 0.042475 [55200/60000]\n",
      "loss: 0.493591 [55800/60000]\n",
      "loss: 0.393462 [56400/60000]\n",
      "loss: 0.459155 [57000/60000]\n",
      "loss: 1.130815 [57600/60000]\n",
      "loss: 0.825545 [58200/60000]\n",
      "loss: 0.377093 [58800/60000]\n",
      "loss: 0.115424 [59400/60000]\n",
      "Test Error: \n",
      " Accuracy: 86.6%, Avg loss: 0.390348 \n",
      "\n",
      "Epoch 5\n",
      "--------------------------------\n",
      "loss: 0.726388 [    0/60000]\n",
      "loss: 0.130728 [  600/60000]\n",
      "loss: 1.227294 [ 1200/60000]\n",
      "loss: 0.094524 [ 1800/60000]\n",
      "loss: 0.388383 [ 2400/60000]\n",
      "loss: 0.162660 [ 3000/60000]\n",
      "loss: 0.173197 [ 3600/60000]\n",
      "loss: 0.264635 [ 4200/60000]\n",
      "loss: 0.630882 [ 4800/60000]\n",
      "loss: 0.389564 [ 5400/60000]\n",
      "loss: 1.003201 [ 6000/60000]\n",
      "loss: 0.250599 [ 6600/60000]\n",
      "loss: 0.606819 [ 7200/60000]\n",
      "loss: 0.504209 [ 7800/60000]\n",
      "loss: 0.013666 [ 8400/60000]\n",
      "loss: 0.018135 [ 9000/60000]\n",
      "loss: 0.535763 [ 9600/60000]\n",
      "loss: 0.126542 [10200/60000]\n",
      "loss: 0.192305 [10800/60000]\n",
      "loss: 0.341088 [11400/60000]\n",
      "loss: 0.263920 [12000/60000]\n",
      "loss: 0.209929 [12600/60000]\n",
      "loss: 0.766808 [13200/60000]\n",
      "loss: 0.071819 [13800/60000]\n",
      "loss: 0.103263 [14400/60000]\n",
      "loss: 0.028208 [15000/60000]\n",
      "loss: 0.016360 [15600/60000]\n",
      "loss: 0.007710 [16200/60000]\n",
      "loss: 0.841954 [16800/60000]\n",
      "loss: 0.015481 [17400/60000]\n",
      "loss: 0.548976 [18000/60000]\n",
      "loss: 0.008415 [18600/60000]\n",
      "loss: 0.021386 [19200/60000]\n",
      "loss: 0.324124 [19800/60000]\n",
      "loss: 0.311827 [20400/60000]\n",
      "loss: 0.037403 [21000/60000]\n",
      "loss: 0.895572 [21600/60000]\n",
      "loss: 0.007246 [22200/60000]\n",
      "loss: 0.100895 [22800/60000]\n",
      "loss: 0.148296 [23400/60000]\n",
      "loss: 0.832070 [24000/60000]\n",
      "loss: 0.115123 [24600/60000]\n",
      "loss: 0.024290 [25200/60000]\n",
      "loss: 0.290352 [25800/60000]\n",
      "loss: 0.085118 [26400/60000]\n",
      "loss: 0.000658 [27000/60000]\n",
      "loss: 0.393350 [27600/60000]\n",
      "loss: 0.136674 [28200/60000]\n",
      "loss: 0.035490 [28800/60000]\n",
      "loss: 0.002946 [29400/60000]\n",
      "loss: 0.005539 [30000/60000]\n",
      "loss: 0.013900 [30600/60000]\n",
      "loss: 0.595850 [31200/60000]\n",
      "loss: 0.015557 [31800/60000]\n",
      "loss: 0.799436 [32400/60000]\n",
      "loss: 0.678783 [33000/60000]\n",
      "loss: 0.296906 [33600/60000]\n",
      "loss: 0.388442 [34200/60000]\n",
      "loss: 1.120915 [34800/60000]\n",
      "loss: 0.088708 [35400/60000]\n",
      "loss: 0.065487 [36000/60000]\n",
      "loss: 0.079098 [36600/60000]\n",
      "loss: 0.004837 [37200/60000]\n",
      "loss: 0.567330 [37800/60000]\n",
      "loss: 0.007090 [38400/60000]\n",
      "loss: 0.901702 [39000/60000]\n",
      "loss: 0.527183 [39600/60000]\n",
      "loss: 0.250996 [40200/60000]\n",
      "loss: 0.270591 [40800/60000]\n",
      "loss: 0.306019 [41400/60000]\n",
      "loss: 0.007817 [42000/60000]\n",
      "loss: 0.574333 [42600/60000]\n",
      "loss: 0.002851 [43200/60000]\n",
      "loss: 0.252679 [43800/60000]\n",
      "loss: 0.305813 [44400/60000]\n",
      "loss: 0.504265 [45000/60000]\n",
      "loss: 0.166646 [45600/60000]\n",
      "loss: 0.198683 [46200/60000]\n",
      "loss: 0.157857 [46800/60000]\n",
      "loss: 0.398340 [47400/60000]\n",
      "loss: 0.060753 [48000/60000]\n",
      "loss: 0.061988 [48600/60000]\n",
      "loss: 0.189455 [49200/60000]\n",
      "loss: 0.050659 [49800/60000]\n",
      "loss: 0.283884 [50400/60000]\n",
      "loss: 0.556688 [51000/60000]\n",
      "loss: 1.057492 [51600/60000]\n",
      "loss: 0.303155 [52200/60000]\n",
      "loss: 0.219229 [52800/60000]\n",
      "loss: 0.306337 [53400/60000]\n",
      "loss: 0.114269 [54000/60000]\n",
      "loss: 0.088952 [54600/60000]\n",
      "loss: 0.086477 [55200/60000]\n",
      "loss: 0.440973 [55800/60000]\n",
      "loss: 0.355841 [56400/60000]\n",
      "loss: 0.340647 [57000/60000]\n",
      "loss: 0.775067 [57600/60000]\n",
      "loss: 0.349594 [58200/60000]\n",
      "loss: 0.354039 [58800/60000]\n",
      "loss: 0.257645 [59400/60000]\n",
      "Test Error: \n",
      " Accuracy: 87.1%, Avg loss: 0.380141 \n",
      "\n",
      "Epoch 6\n",
      "--------------------------------\n",
      "loss: 0.424391 [    0/60000]\n",
      "loss: 0.269593 [  600/60000]\n",
      "loss: 0.953074 [ 1200/60000]\n",
      "loss: 0.063930 [ 1800/60000]\n",
      "loss: 0.135523 [ 2400/60000]\n",
      "loss: 0.356088 [ 3000/60000]\n",
      "loss: 0.467358 [ 3600/60000]\n",
      "loss: 0.255226 [ 4200/60000]\n",
      "loss: 1.297719 [ 4800/60000]\n",
      "loss: 0.534466 [ 5400/60000]\n",
      "loss: 0.801764 [ 6000/60000]\n",
      "loss: 0.182980 [ 6600/60000]\n",
      "loss: 0.782658 [ 7200/60000]\n",
      "loss: 0.566784 [ 7800/60000]\n",
      "loss: 0.013275 [ 8400/60000]\n",
      "loss: 0.015854 [ 9000/60000]\n",
      "loss: 0.451418 [ 9600/60000]\n",
      "loss: 0.046567 [10200/60000]\n",
      "loss: 0.167699 [10800/60000]\n",
      "loss: 0.382641 [11400/60000]\n",
      "loss: 0.305640 [12000/60000]\n",
      "loss: 0.248030 [12600/60000]\n",
      "loss: 0.797314 [13200/60000]\n",
      "loss: 0.093216 [13800/60000]\n",
      "loss: 0.169700 [14400/60000]\n",
      "loss: 0.036980 [15000/60000]\n",
      "loss: 0.012657 [15600/60000]\n",
      "loss: 0.005011 [16200/60000]\n",
      "loss: 0.819133 [16800/60000]\n",
      "loss: 0.022353 [17400/60000]\n",
      "loss: 0.528070 [18000/60000]\n",
      "loss: 0.016416 [18600/60000]\n",
      "loss: 0.018080 [19200/60000]\n",
      "loss: 0.275428 [19800/60000]\n",
      "loss: 0.526084 [20400/60000]\n",
      "loss: 0.065746 [21000/60000]\n",
      "loss: 0.577397 [21600/60000]\n",
      "loss: 0.002045 [22200/60000]\n",
      "loss: 0.026375 [22800/60000]\n",
      "loss: 0.389683 [23400/60000]\n",
      "loss: 0.774221 [24000/60000]\n",
      "loss: 0.150263 [24600/60000]\n",
      "loss: 0.035471 [25200/60000]\n",
      "loss: 0.561194 [25800/60000]\n",
      "loss: 0.041167 [26400/60000]\n",
      "loss: 0.000246 [27000/60000]\n",
      "loss: 0.290580 [27600/60000]\n",
      "loss: 0.167551 [28200/60000]\n",
      "loss: 0.041722 [28800/60000]\n",
      "loss: 0.003279 [29400/60000]\n",
      "loss: 0.077731 [30000/60000]\n",
      "loss: 0.024954 [30600/60000]\n",
      "loss: 0.579266 [31200/60000]\n",
      "loss: 0.001659 [31800/60000]\n",
      "loss: 0.226121 [32400/60000]\n",
      "loss: 0.672075 [33000/60000]\n",
      "loss: 0.199821 [33600/60000]\n",
      "loss: 0.312941 [34200/60000]\n",
      "loss: 1.027197 [34800/60000]\n",
      "loss: 0.094441 [35400/60000]\n",
      "loss: 0.129047 [36000/60000]\n",
      "loss: 0.062092 [36600/60000]\n",
      "loss: 0.007588 [37200/60000]\n",
      "loss: 0.151876 [37800/60000]\n",
      "loss: 0.087621 [38400/60000]\n",
      "loss: 0.375056 [39000/60000]\n",
      "loss: 0.128576 [39600/60000]\n",
      "loss: 0.319021 [40200/60000]\n",
      "loss: 0.224389 [40800/60000]\n",
      "loss: 0.188855 [41400/60000]\n",
      "loss: 0.017866 [42000/60000]\n",
      "loss: 0.716772 [42600/60000]\n",
      "loss: 0.005296 [43200/60000]\n",
      "loss: 0.132405 [43800/60000]\n",
      "loss: 0.295051 [44400/60000]\n",
      "loss: 0.627618 [45000/60000]\n",
      "loss: 0.222345 [45600/60000]\n",
      "loss: 0.104554 [46200/60000]\n",
      "loss: 0.210823 [46800/60000]\n",
      "loss: 0.358085 [47400/60000]\n",
      "loss: 0.023084 [48000/60000]\n",
      "loss: 0.060975 [48600/60000]\n",
      "loss: 0.192583 [49200/60000]\n",
      "loss: 0.025535 [49800/60000]\n",
      "loss: 0.361301 [50400/60000]\n",
      "loss: 0.428298 [51000/60000]\n",
      "loss: 1.512400 [51600/60000]\n",
      "loss: 0.192208 [52200/60000]\n",
      "loss: 0.307193 [52800/60000]\n",
      "loss: 0.359088 [53400/60000]\n",
      "loss: 0.128766 [54000/60000]\n",
      "loss: 0.061132 [54600/60000]\n",
      "loss: 0.043552 [55200/60000]\n",
      "loss: 0.381757 [55800/60000]\n",
      "loss: 0.222666 [56400/60000]\n",
      "loss: 0.441476 [57000/60000]\n",
      "loss: 0.680868 [57600/60000]\n",
      "loss: 0.115966 [58200/60000]\n",
      "loss: 0.248049 [58800/60000]\n",
      "loss: 0.153921 [59400/60000]\n",
      "Test Error: \n",
      " Accuracy: 87.0%, Avg loss: 0.384487 \n",
      "\n",
      "Epoch 7\n",
      "--------------------------------\n",
      "loss: 0.677648 [    0/60000]\n",
      "loss: 0.289920 [  600/60000]\n",
      "loss: 1.263208 [ 1200/60000]\n",
      "loss: 0.047118 [ 1800/60000]\n",
      "loss: 0.627614 [ 2400/60000]\n",
      "loss: 0.160596 [ 3000/60000]\n",
      "loss: 0.254144 [ 3600/60000]\n",
      "loss: 0.298114 [ 4200/60000]\n",
      "loss: 0.673759 [ 4800/60000]\n",
      "loss: 0.702866 [ 5400/60000]\n",
      "loss: 0.983440 [ 6000/60000]\n",
      "loss: 0.203791 [ 6600/60000]\n",
      "loss: 0.732863 [ 7200/60000]\n",
      "loss: 0.493460 [ 7800/60000]\n",
      "loss: 0.016944 [ 8400/60000]\n",
      "loss: 0.081682 [ 9000/60000]\n",
      "loss: 0.574677 [ 9600/60000]\n",
      "loss: 0.044126 [10200/60000]\n",
      "loss: 0.416343 [10800/60000]\n",
      "loss: 0.340164 [11400/60000]\n",
      "loss: 0.218963 [12000/60000]\n",
      "loss: 0.196312 [12600/60000]\n",
      "loss: 0.789846 [13200/60000]\n",
      "loss: 0.101584 [13800/60000]\n",
      "loss: 0.080671 [14400/60000]\n",
      "loss: 0.073217 [15000/60000]\n",
      "loss: 0.009939 [15600/60000]\n",
      "loss: 0.009275 [16200/60000]\n",
      "loss: 0.914341 [16800/60000]\n",
      "loss: 0.018747 [17400/60000]\n",
      "loss: 0.461301 [18000/60000]\n",
      "loss: 0.006404 [18600/60000]\n",
      "loss: 0.021199 [19200/60000]\n",
      "loss: 0.200194 [19800/60000]\n",
      "loss: 0.151987 [20400/60000]\n",
      "loss: 0.007315 [21000/60000]\n",
      "loss: 0.968573 [21600/60000]\n",
      "loss: 0.002494 [22200/60000]\n",
      "loss: 0.037948 [22800/60000]\n",
      "loss: 0.031360 [23400/60000]\n",
      "loss: 0.589411 [24000/60000]\n",
      "loss: 0.140940 [24600/60000]\n",
      "loss: 0.013036 [25200/60000]\n",
      "loss: 0.444101 [25800/60000]\n",
      "loss: 0.014138 [26400/60000]\n",
      "loss: 0.000426 [27000/60000]\n",
      "loss: 0.229711 [27600/60000]\n",
      "loss: 0.149168 [28200/60000]\n",
      "loss: 0.035186 [28800/60000]\n",
      "loss: 0.001439 [29400/60000]\n",
      "loss: 0.034525 [30000/60000]\n",
      "loss: 0.011193 [30600/60000]\n",
      "loss: 0.634391 [31200/60000]\n",
      "loss: 0.008677 [31800/60000]\n",
      "loss: 0.680516 [32400/60000]\n",
      "loss: 0.694353 [33000/60000]\n",
      "loss: 0.213227 [33600/60000]\n",
      "loss: 0.234846 [34200/60000]\n",
      "loss: 0.981614 [34800/60000]\n",
      "loss: 0.095876 [35400/60000]\n",
      "loss: 0.050660 [36000/60000]\n",
      "loss: 0.018350 [36600/60000]\n",
      "loss: 0.026388 [37200/60000]\n",
      "loss: 0.136065 [37800/60000]\n",
      "loss: 0.000729 [38400/60000]\n",
      "loss: 0.482132 [39000/60000]\n",
      "loss: 0.213473 [39600/60000]\n",
      "loss: 0.195622 [40200/60000]\n",
      "loss: 0.412111 [40800/60000]\n",
      "loss: 0.212028 [41400/60000]\n",
      "loss: 0.014176 [42000/60000]\n",
      "loss: 0.604237 [42600/60000]\n",
      "loss: 0.002698 [43200/60000]\n",
      "loss: 0.152935 [43800/60000]\n",
      "loss: 0.319261 [44400/60000]\n",
      "loss: 0.336733 [45000/60000]\n",
      "loss: 0.159232 [45600/60000]\n",
      "loss: 0.159695 [46200/60000]\n",
      "loss: 0.168777 [46800/60000]\n",
      "loss: 0.223805 [47400/60000]\n",
      "loss: 0.046870 [48000/60000]\n",
      "loss: 0.020902 [48600/60000]\n",
      "loss: 0.118745 [49200/60000]\n",
      "loss: 0.050540 [49800/60000]\n",
      "loss: 0.230657 [50400/60000]\n",
      "loss: 0.289272 [51000/60000]\n",
      "loss: 0.920775 [51600/60000]\n",
      "loss: 0.529000 [52200/60000]\n",
      "loss: 0.197491 [52800/60000]\n",
      "loss: 0.331706 [53400/60000]\n",
      "loss: 0.097383 [54000/60000]\n",
      "loss: 0.101985 [54600/60000]\n",
      "loss: 0.089807 [55200/60000]\n",
      "loss: 0.393398 [55800/60000]\n",
      "loss: 0.340731 [56400/60000]\n",
      "loss: 0.439075 [57000/60000]\n",
      "loss: 0.551574 [57600/60000]\n",
      "loss: 0.384632 [58200/60000]\n",
      "loss: 0.315939 [58800/60000]\n",
      "loss: 0.310923 [59400/60000]\n",
      "Test Error: \n",
      " Accuracy: 86.9%, Avg loss: 0.410468 \n",
      "\n",
      "Epoch 8\n",
      "--------------------------------\n",
      "loss: 0.344119 [    0/60000]\n",
      "loss: 0.044523 [  600/60000]\n",
      "loss: 0.994079 [ 1200/60000]\n",
      "loss: 0.060006 [ 1800/60000]\n",
      "loss: 0.238886 [ 2400/60000]\n",
      "loss: 0.171379 [ 3000/60000]\n",
      "loss: 0.175346 [ 3600/60000]\n",
      "loss: 0.490258 [ 4200/60000]\n",
      "loss: 0.080833 [ 4800/60000]\n",
      "loss: 0.608054 [ 5400/60000]\n",
      "loss: 0.992653 [ 6000/60000]\n",
      "loss: 0.412624 [ 6600/60000]\n",
      "loss: 0.710367 [ 7200/60000]\n",
      "loss: 0.473401 [ 7800/60000]\n",
      "loss: 0.013350 [ 8400/60000]\n",
      "loss: 0.048870 [ 9000/60000]\n",
      "loss: 0.560016 [ 9600/60000]\n",
      "loss: 0.030498 [10200/60000]\n",
      "loss: 0.540979 [10800/60000]\n",
      "loss: 0.320922 [11400/60000]\n",
      "loss: 0.255242 [12000/60000]\n",
      "loss: 0.227794 [12600/60000]\n",
      "loss: 0.512817 [13200/60000]\n",
      "loss: 0.040368 [13800/60000]\n",
      "loss: 0.076825 [14400/60000]\n",
      "loss: 0.042482 [15000/60000]\n",
      "loss: 0.013713 [15600/60000]\n",
      "loss: 0.037754 [16200/60000]\n",
      "loss: 0.855465 [16800/60000]\n",
      "loss: 0.016145 [17400/60000]\n",
      "loss: 0.529550 [18000/60000]\n",
      "loss: 0.017667 [18600/60000]\n",
      "loss: 0.029063 [19200/60000]\n",
      "loss: 0.266727 [19800/60000]\n",
      "loss: 0.360696 [20400/60000]\n",
      "loss: 0.021537 [21000/60000]\n",
      "loss: 0.611443 [21600/60000]\n",
      "loss: 0.003016 [22200/60000]\n",
      "loss: 0.078345 [22800/60000]\n",
      "loss: 0.076548 [23400/60000]\n",
      "loss: 0.429442 [24000/60000]\n",
      "loss: 0.034743 [24600/60000]\n",
      "loss: 0.036888 [25200/60000]\n",
      "loss: 0.405928 [25800/60000]\n",
      "loss: 0.026374 [26400/60000]\n",
      "loss: 0.000704 [27000/60000]\n",
      "loss: 0.228206 [27600/60000]\n",
      "loss: 0.177561 [28200/60000]\n",
      "loss: 0.026411 [28800/60000]\n",
      "loss: 0.012924 [29400/60000]\n",
      "loss: 0.245476 [30000/60000]\n",
      "loss: 0.018532 [30600/60000]\n",
      "loss: 0.783747 [31200/60000]\n",
      "loss: 0.005034 [31800/60000]\n",
      "loss: 0.187088 [32400/60000]\n",
      "loss: 0.537436 [33000/60000]\n",
      "loss: 0.174368 [33600/60000]\n",
      "loss: 0.225049 [34200/60000]\n",
      "loss: 0.976427 [34800/60000]\n",
      "loss: 0.077956 [35400/60000]\n",
      "loss: 0.048235 [36000/60000]\n",
      "loss: 0.067421 [36600/60000]\n",
      "loss: 0.026653 [37200/60000]\n",
      "loss: 0.081742 [37800/60000]\n",
      "loss: 0.018684 [38400/60000]\n",
      "loss: 0.429576 [39000/60000]\n",
      "loss: 0.051632 [39600/60000]\n",
      "loss: 0.180453 [40200/60000]\n",
      "loss: 0.355739 [40800/60000]\n",
      "loss: 0.156551 [41400/60000]\n",
      "loss: 0.045259 [42000/60000]\n",
      "loss: 0.644935 [42600/60000]\n",
      "loss: 0.005095 [43200/60000]\n",
      "loss: 0.153316 [43800/60000]\n",
      "loss: 0.274562 [44400/60000]\n",
      "loss: 0.404201 [45000/60000]\n",
      "loss: 0.192879 [45600/60000]\n",
      "loss: 0.085607 [46200/60000]\n",
      "loss: 0.070021 [46800/60000]\n",
      "loss: 0.296445 [47400/60000]\n",
      "loss: 0.007194 [48000/60000]\n",
      "loss: 0.020560 [48600/60000]\n",
      "loss: 0.147280 [49200/60000]\n",
      "loss: 0.011122 [49800/60000]\n",
      "loss: 0.345384 [50400/60000]\n",
      "loss: 0.254593 [51000/60000]\n",
      "loss: 1.035654 [51600/60000]\n",
      "loss: 0.325627 [52200/60000]\n",
      "loss: 0.236247 [52800/60000]\n",
      "loss: 0.264480 [53400/60000]\n",
      "loss: 0.101975 [54000/60000]\n",
      "loss: 0.109202 [54600/60000]\n",
      "loss: 0.039329 [55200/60000]\n",
      "loss: 0.571736 [55800/60000]\n",
      "loss: 0.496440 [56400/60000]\n",
      "loss: 0.505531 [57000/60000]\n",
      "loss: 0.957906 [57600/60000]\n",
      "loss: 1.080190 [58200/60000]\n",
      "loss: 0.300498 [58800/60000]\n",
      "loss: 0.209623 [59400/60000]\n",
      "Test Error: \n",
      " Accuracy: 86.8%, Avg loss: 0.427202 \n",
      "\n",
      "Epoch 9\n",
      "--------------------------------\n",
      "loss: 0.699692 [    0/60000]\n",
      "loss: 0.104938 [  600/60000]\n",
      "loss: 1.195384 [ 1200/60000]\n",
      "loss: 0.052051 [ 1800/60000]\n",
      "loss: 0.362440 [ 2400/60000]\n",
      "loss: 0.286984 [ 3000/60000]\n",
      "loss: 0.139943 [ 3600/60000]\n",
      "loss: 0.076329 [ 4200/60000]\n",
      "loss: 1.113703 [ 4800/60000]\n",
      "loss: 0.349517 [ 5400/60000]\n",
      "loss: 1.079957 [ 6000/60000]\n",
      "loss: 0.184466 [ 6600/60000]\n",
      "loss: 0.884754 [ 7200/60000]\n",
      "loss: 0.417699 [ 7800/60000]\n",
      "loss: 0.016580 [ 8400/60000]\n",
      "loss: 0.016195 [ 9000/60000]\n",
      "loss: 0.422647 [ 9600/60000]\n",
      "loss: 0.031510 [10200/60000]\n",
      "loss: 0.259062 [10800/60000]\n",
      "loss: 0.302864 [11400/60000]\n",
      "loss: 0.314395 [12000/60000]\n",
      "loss: 0.132234 [12600/60000]\n",
      "loss: 0.659106 [13200/60000]\n",
      "loss: 0.026814 [13800/60000]\n",
      "loss: 0.054205 [14400/60000]\n",
      "loss: 0.084740 [15000/60000]\n",
      "loss: 0.052484 [15600/60000]\n",
      "loss: 0.033924 [16200/60000]\n",
      "loss: 0.795199 [16800/60000]\n",
      "loss: 0.022775 [17400/60000]\n",
      "loss: 0.400816 [18000/60000]\n",
      "loss: 0.004721 [18600/60000]\n",
      "loss: 0.017586 [19200/60000]\n",
      "loss: 0.232684 [19800/60000]\n",
      "loss: 0.207008 [20400/60000]\n",
      "loss: 0.028130 [21000/60000]\n",
      "loss: 0.911205 [21600/60000]\n",
      "loss: 0.002097 [22200/60000]\n",
      "loss: 0.104485 [22800/60000]\n",
      "loss: 0.146116 [23400/60000]\n",
      "loss: 0.534947 [24000/60000]\n",
      "loss: 0.089446 [24600/60000]\n",
      "loss: 0.047606 [25200/60000]\n",
      "loss: 0.384946 [25800/60000]\n",
      "loss: 0.023217 [26400/60000]\n",
      "loss: 0.000503 [27000/60000]\n",
      "loss: 0.220026 [27600/60000]\n",
      "loss: 0.163192 [28200/60000]\n",
      "loss: 0.027461 [28800/60000]\n",
      "loss: 0.012126 [29400/60000]\n",
      "loss: 0.087180 [30000/60000]\n",
      "loss: 0.015853 [30600/60000]\n",
      "loss: 0.623117 [31200/60000]\n",
      "loss: 0.019145 [31800/60000]\n",
      "loss: 0.177411 [32400/60000]\n",
      "loss: 0.575588 [33000/60000]\n",
      "loss: 0.184262 [33600/60000]\n",
      "loss: 0.292760 [34200/60000]\n",
      "loss: 0.850322 [34800/60000]\n",
      "loss: 0.075735 [35400/60000]\n",
      "loss: 0.119641 [36000/60000]\n",
      "loss: 0.039173 [36600/60000]\n",
      "loss: 0.008525 [37200/60000]\n",
      "loss: 0.106167 [37800/60000]\n",
      "loss: 0.283371 [38400/60000]\n",
      "loss: 0.579212 [39000/60000]\n",
      "loss: 0.216247 [39600/60000]\n",
      "loss: 0.264915 [40200/60000]\n",
      "loss: 0.148560 [40800/60000]\n",
      "loss: 0.368918 [41400/60000]\n",
      "loss: 0.024595 [42000/60000]\n",
      "loss: 0.545684 [42600/60000]\n",
      "loss: 0.002756 [43200/60000]\n",
      "loss: 0.088661 [43800/60000]\n",
      "loss: 0.244074 [44400/60000]\n",
      "loss: 0.460033 [45000/60000]\n",
      "loss: 0.204107 [45600/60000]\n",
      "loss: 0.123857 [46200/60000]\n",
      "loss: 0.113262 [46800/60000]\n",
      "loss: 0.275727 [47400/60000]\n",
      "loss: 0.032212 [48000/60000]\n",
      "loss: 0.009241 [48600/60000]\n",
      "loss: 0.006935 [49200/60000]\n",
      "loss: 0.019517 [49800/60000]\n",
      "loss: 0.532576 [50400/60000]\n",
      "loss: 0.178427 [51000/60000]\n",
      "loss: 1.589426 [51600/60000]\n",
      "loss: 0.214054 [52200/60000]\n",
      "loss: 0.171447 [52800/60000]\n",
      "loss: 0.293339 [53400/60000]\n",
      "loss: 0.127664 [54000/60000]\n",
      "loss: 0.130064 [54600/60000]\n",
      "loss: 0.018868 [55200/60000]\n",
      "loss: 0.328836 [55800/60000]\n",
      "loss: 0.361046 [56400/60000]\n",
      "loss: 0.529974 [57000/60000]\n",
      "loss: 0.581034 [57600/60000]\n",
      "loss: 0.274057 [58200/60000]\n",
      "loss: 0.290667 [58800/60000]\n",
      "loss: 0.248450 [59400/60000]\n",
      "Test Error: \n",
      " Accuracy: 86.9%, Avg loss: 0.494951 \n",
      "\n",
      "Epoch 10\n",
      "--------------------------------\n",
      "loss: 0.444828 [    0/60000]\n",
      "loss: 0.313011 [  600/60000]\n",
      "loss: 0.736802 [ 1200/60000]\n",
      "loss: 0.102048 [ 1800/60000]\n",
      "loss: 0.636310 [ 2400/60000]\n",
      "loss: 0.471352 [ 3000/60000]\n",
      "loss: 0.504765 [ 3600/60000]\n",
      "loss: 0.053738 [ 4200/60000]\n",
      "loss: 0.114477 [ 4800/60000]\n",
      "loss: 0.487432 [ 5400/60000]\n",
      "loss: 1.027875 [ 6000/60000]\n",
      "loss: 0.487528 [ 6600/60000]\n",
      "loss: 1.047984 [ 7200/60000]\n",
      "loss: 0.419015 [ 7800/60000]\n",
      "loss: 0.024975 [ 8400/60000]\n",
      "loss: 0.027013 [ 9000/60000]\n",
      "loss: 0.568660 [ 9600/60000]\n",
      "loss: 0.014092 [10200/60000]\n",
      "loss: 0.632633 [10800/60000]\n",
      "loss: 0.366467 [11400/60000]\n",
      "loss: 0.350051 [12000/60000]\n",
      "loss: 0.203595 [12600/60000]\n",
      "loss: 0.374505 [13200/60000]\n",
      "loss: 0.063085 [13800/60000]\n",
      "loss: 0.079755 [14400/60000]\n",
      "loss: 0.046737 [15000/60000]\n",
      "loss: 0.034600 [15600/60000]\n",
      "loss: 0.032212 [16200/60000]\n",
      "loss: 0.947767 [16800/60000]\n",
      "loss: 0.016857 [17400/60000]\n",
      "loss: 0.419017 [18000/60000]\n",
      "loss: 0.005960 [18600/60000]\n",
      "loss: 0.019998 [19200/60000]\n",
      "loss: 0.255340 [19800/60000]\n",
      "loss: 0.288371 [20400/60000]\n",
      "loss: 0.017812 [21000/60000]\n",
      "loss: 0.618569 [21600/60000]\n",
      "loss: 0.003909 [22200/60000]\n",
      "loss: 0.090575 [22800/60000]\n",
      "loss: 0.025189 [23400/60000]\n",
      "loss: 0.569530 [24000/60000]\n",
      "loss: 0.040445 [24600/60000]\n",
      "loss: 0.021428 [25200/60000]\n",
      "loss: 0.357603 [25800/60000]\n",
      "loss: 0.031787 [26400/60000]\n",
      "loss: 0.000468 [27000/60000]\n",
      "loss: 0.738002 [27600/60000]\n",
      "loss: 0.174811 [28200/60000]\n",
      "loss: 0.034912 [28800/60000]\n",
      "loss: 0.003924 [29400/60000]\n",
      "loss: 0.006764 [30000/60000]\n",
      "loss: 0.016430 [30600/60000]\n",
      "loss: 0.612420 [31200/60000]\n",
      "loss: 0.015626 [31800/60000]\n",
      "loss: 0.565020 [32400/60000]\n",
      "loss: 0.557937 [33000/60000]\n",
      "loss: 0.111307 [33600/60000]\n",
      "loss: 0.289268 [34200/60000]\n",
      "loss: 1.036457 [34800/60000]\n",
      "loss: 0.120484 [35400/60000]\n",
      "loss: 0.192658 [36000/60000]\n",
      "loss: 0.035527 [36600/60000]\n",
      "loss: 0.003478 [37200/60000]\n",
      "loss: 0.078119 [37800/60000]\n",
      "loss: 0.001830 [38400/60000]\n",
      "loss: 0.413118 [39000/60000]\n",
      "loss: 0.152649 [39600/60000]\n",
      "loss: 0.149901 [40200/60000]\n",
      "loss: 0.209149 [40800/60000]\n",
      "loss: 0.288173 [41400/60000]\n",
      "loss: 0.015236 [42000/60000]\n",
      "loss: 0.924800 [42600/60000]\n",
      "loss: 0.001917 [43200/60000]\n",
      "loss: 0.050492 [43800/60000]\n",
      "loss: 0.397332 [44400/60000]\n",
      "loss: 0.499936 [45000/60000]\n",
      "loss: 0.155915 [45600/60000]\n",
      "loss: 0.151368 [46200/60000]\n",
      "loss: 0.110551 [46800/60000]\n",
      "loss: 0.179077 [47400/60000]\n",
      "loss: 0.013806 [48000/60000]\n",
      "loss: 0.005381 [48600/60000]\n",
      "loss: 0.010571 [49200/60000]\n",
      "loss: 0.039526 [49800/60000]\n",
      "loss: 0.067523 [50400/60000]\n",
      "loss: 0.497372 [51000/60000]\n",
      "loss: 1.018208 [51600/60000]\n",
      "loss: 0.328131 [52200/60000]\n",
      "loss: 0.125919 [52800/60000]\n",
      "loss: 0.339476 [53400/60000]\n",
      "loss: 0.080975 [54000/60000]\n",
      "loss: 0.119616 [54600/60000]\n",
      "loss: 0.084014 [55200/60000]\n",
      "loss: 0.376767 [55800/60000]\n",
      "loss: 0.185059 [56400/60000]\n",
      "loss: 0.431062 [57000/60000]\n",
      "loss: 0.459337 [57600/60000]\n",
      "loss: 0.205514 [58200/60000]\n",
      "loss: 0.300729 [58800/60000]\n",
      "loss: 0.099073 [59400/60000]\n",
      "Test Error: \n",
      " Accuracy: 87.5%, Avg loss: 0.441247 \n",
      "\n",
      "Epoch 11\n",
      "--------------------------------\n",
      "loss: 0.415981 [    0/60000]\n",
      "loss: 0.042957 [  600/60000]\n",
      "loss: 0.730443 [ 1200/60000]\n",
      "loss: 0.047753 [ 1800/60000]\n",
      "loss: 0.235364 [ 2400/60000]\n",
      "loss: 0.513417 [ 3000/60000]\n",
      "loss: 0.068375 [ 3600/60000]\n",
      "loss: 0.385950 [ 4200/60000]\n",
      "loss: 1.030596 [ 4800/60000]\n",
      "loss: 0.313709 [ 5400/60000]\n",
      "loss: 1.037517 [ 6000/60000]\n",
      "loss: 0.274733 [ 6600/60000]\n",
      "loss: 1.089117 [ 7200/60000]\n",
      "loss: 0.449138 [ 7800/60000]\n",
      "loss: 0.055192 [ 8400/60000]\n",
      "loss: 0.018205 [ 9000/60000]\n",
      "loss: 0.677028 [ 9600/60000]\n",
      "loss: 0.019849 [10200/60000]\n",
      "loss: 0.128413 [10800/60000]\n",
      "loss: 0.462127 [11400/60000]\n",
      "loss: 0.446330 [12000/60000]\n",
      "loss: 0.225066 [12600/60000]\n",
      "loss: 0.299326 [13200/60000]\n",
      "loss: 0.038271 [13800/60000]\n",
      "loss: 0.050959 [14400/60000]\n",
      "loss: 0.036144 [15000/60000]\n",
      "loss: 0.074312 [15600/60000]\n",
      "loss: 0.020290 [16200/60000]\n",
      "loss: 0.775586 [16800/60000]\n",
      "loss: 0.020480 [17400/60000]\n",
      "loss: 0.305554 [18000/60000]\n",
      "loss: 0.002744 [18600/60000]\n",
      "loss: 0.044146 [19200/60000]\n",
      "loss: 0.286031 [19800/60000]\n",
      "loss: 0.212572 [20400/60000]\n",
      "loss: 0.013958 [21000/60000]\n",
      "loss: 0.946800 [21600/60000]\n",
      "loss: 0.002404 [22200/60000]\n",
      "loss: 0.047281 [22800/60000]\n",
      "loss: 0.091614 [23400/60000]\n",
      "loss: 0.522787 [24000/60000]\n",
      "loss: 0.071991 [24600/60000]\n",
      "loss: 0.050491 [25200/60000]\n",
      "loss: 0.570410 [25800/60000]\n",
      "loss: 0.053587 [26400/60000]\n",
      "loss: 0.000724 [27000/60000]\n",
      "loss: 0.229106 [27600/60000]\n",
      "loss: 0.076199 [28200/60000]\n",
      "loss: 0.036969 [28800/60000]\n",
      "loss: 0.003654 [29400/60000]\n",
      "loss: 0.009722 [30000/60000]\n",
      "loss: 0.047144 [30600/60000]\n",
      "loss: 0.606825 [31200/60000]\n",
      "loss: 0.015851 [31800/60000]\n",
      "loss: 0.258617 [32400/60000]\n",
      "loss: 0.468923 [33000/60000]\n",
      "loss: 0.059106 [33600/60000]\n",
      "loss: 0.309442 [34200/60000]\n",
      "loss: 0.990964 [34800/60000]\n",
      "loss: 0.097205 [35400/60000]\n",
      "loss: 0.026455 [36000/60000]\n",
      "loss: 0.028561 [36600/60000]\n",
      "loss: 0.002164 [37200/60000]\n",
      "loss: 0.122070 [37800/60000]\n",
      "loss: 0.001939 [38400/60000]\n",
      "loss: 0.515612 [39000/60000]\n",
      "loss: 0.187748 [39600/60000]\n",
      "loss: 0.202935 [40200/60000]\n",
      "loss: 0.141608 [40800/60000]\n",
      "loss: 0.239874 [41400/60000]\n",
      "loss: 0.043308 [42000/60000]\n",
      "loss: 0.759242 [42600/60000]\n",
      "loss: 0.000586 [43200/60000]\n",
      "loss: 0.219662 [43800/60000]\n",
      "loss: 0.242490 [44400/60000]\n",
      "loss: 0.262380 [45000/60000]\n",
      "loss: 0.088441 [45600/60000]\n",
      "loss: 0.119939 [46200/60000]\n",
      "loss: 0.102874 [46800/60000]\n",
      "loss: 0.259078 [47400/60000]\n",
      "loss: 0.151177 [48000/60000]\n",
      "loss: 0.002399 [48600/60000]\n",
      "loss: 0.073883 [49200/60000]\n",
      "loss: 0.016412 [49800/60000]\n",
      "loss: 0.361102 [50400/60000]\n",
      "loss: 0.311740 [51000/60000]\n",
      "loss: 0.838604 [51600/60000]\n",
      "loss: 0.461211 [52200/60000]\n",
      "loss: 0.145011 [52800/60000]\n",
      "loss: 0.294696 [53400/60000]\n",
      "loss: 0.072320 [54000/60000]\n",
      "loss: 0.623025 [54600/60000]\n",
      "loss: 0.179358 [55200/60000]\n",
      "loss: 0.268922 [55800/60000]\n",
      "loss: 0.197510 [56400/60000]\n",
      "loss: 0.373370 [57000/60000]\n",
      "loss: 0.469602 [57600/60000]\n",
      "loss: 0.617438 [58200/60000]\n",
      "loss: 0.304604 [58800/60000]\n",
      "loss: 0.098073 [59400/60000]\n",
      "Test Error: \n",
      " Accuracy: 87.6%, Avg loss: 0.450781 \n",
      "\n",
      "Epoch 12\n",
      "--------------------------------\n",
      "loss: 0.256529 [    0/60000]\n",
      "loss: 0.050866 [  600/60000]\n",
      "loss: 0.772234 [ 1200/60000]\n",
      "loss: 0.082540 [ 1800/60000]\n",
      "loss: 0.306779 [ 2400/60000]\n",
      "loss: 0.575037 [ 3000/60000]\n",
      "loss: 0.114698 [ 3600/60000]\n",
      "loss: 0.307812 [ 4200/60000]\n",
      "loss: 1.080421 [ 4800/60000]\n",
      "loss: 0.670830 [ 5400/60000]\n",
      "loss: 0.694412 [ 6000/60000]\n",
      "loss: 0.121910 [ 6600/60000]\n",
      "loss: 0.507240 [ 7200/60000]\n",
      "loss: 0.330064 [ 7800/60000]\n",
      "loss: 0.018367 [ 8400/60000]\n",
      "loss: 0.032815 [ 9000/60000]\n",
      "loss: 0.440058 [ 9600/60000]\n",
      "loss: 0.032470 [10200/60000]\n",
      "loss: 0.437628 [10800/60000]\n",
      "loss: 0.521256 [11400/60000]\n",
      "loss: 0.358436 [12000/60000]\n",
      "loss: 0.275689 [12600/60000]\n",
      "loss: 0.590638 [13200/60000]\n",
      "loss: 0.062409 [13800/60000]\n",
      "loss: 0.092757 [14400/60000]\n",
      "loss: 0.020746 [15000/60000]\n",
      "loss: 0.085933 [15600/60000]\n",
      "loss: 0.058620 [16200/60000]\n",
      "loss: 0.793075 [16800/60000]\n",
      "loss: 0.063403 [17400/60000]\n",
      "loss: 0.374936 [18000/60000]\n",
      "loss: 0.007004 [18600/60000]\n",
      "loss: 0.045499 [19200/60000]\n",
      "loss: 0.335791 [19800/60000]\n",
      "loss: 0.275990 [20400/60000]\n",
      "loss: 0.021223 [21000/60000]\n",
      "loss: 0.888629 [21600/60000]\n",
      "loss: 0.002376 [22200/60000]\n",
      "loss: 0.016330 [22800/60000]\n",
      "loss: 0.006384 [23400/60000]\n",
      "loss: 0.515758 [24000/60000]\n",
      "loss: 0.127095 [24600/60000]\n",
      "loss: 0.035103 [25200/60000]\n",
      "loss: 0.298179 [25800/60000]\n",
      "loss: 0.018358 [26400/60000]\n",
      "loss: 0.000860 [27000/60000]\n",
      "loss: 0.110624 [27600/60000]\n",
      "loss: 0.073720 [28200/60000]\n",
      "loss: 0.060607 [28800/60000]\n",
      "loss: 0.004134 [29400/60000]\n",
      "loss: 0.009954 [30000/60000]\n",
      "loss: 0.020719 [30600/60000]\n",
      "loss: 0.635270 [31200/60000]\n",
      "loss: 0.015898 [31800/60000]\n",
      "loss: 0.344737 [32400/60000]\n",
      "loss: 0.744470 [33000/60000]\n",
      "loss: 0.024153 [33600/60000]\n",
      "loss: 0.284121 [34200/60000]\n",
      "loss: 1.107266 [34800/60000]\n",
      "loss: 0.063279 [35400/60000]\n",
      "loss: 0.031012 [36000/60000]\n",
      "loss: 0.037592 [36600/60000]\n",
      "loss: 0.003338 [37200/60000]\n",
      "loss: 0.112050 [37800/60000]\n",
      "loss: 0.000637 [38400/60000]\n",
      "loss: 0.742278 [39000/60000]\n",
      "loss: 0.232314 [39600/60000]\n",
      "loss: 0.158823 [40200/60000]\n",
      "loss: 0.204682 [40800/60000]\n",
      "loss: 0.643955 [41400/60000]\n",
      "loss: 0.011032 [42000/60000]\n",
      "loss: 0.479268 [42600/60000]\n",
      "loss: 0.000236 [43200/60000]\n",
      "loss: 0.139115 [43800/60000]\n",
      "loss: 0.229981 [44400/60000]\n",
      "loss: 0.461361 [45000/60000]\n",
      "loss: 0.162710 [45600/60000]\n",
      "loss: 0.134151 [46200/60000]\n",
      "loss: 0.124381 [46800/60000]\n",
      "loss: 0.246578 [47400/60000]\n",
      "loss: 0.024017 [48000/60000]\n",
      "loss: 0.008014 [48600/60000]\n",
      "loss: 0.023427 [49200/60000]\n",
      "loss: 0.082213 [49800/60000]\n",
      "loss: 0.433056 [50400/60000]\n",
      "loss: 0.152142 [51000/60000]\n",
      "loss: 1.476968 [51600/60000]\n",
      "loss: 0.282932 [52200/60000]\n",
      "loss: 0.115439 [52800/60000]\n",
      "loss: 0.199456 [53400/60000]\n",
      "loss: 0.048293 [54000/60000]\n",
      "loss: 0.156855 [54600/60000]\n",
      "loss: 0.121148 [55200/60000]\n",
      "loss: 0.192129 [55800/60000]\n",
      "loss: 0.134746 [56400/60000]\n",
      "loss: 0.466942 [57000/60000]\n",
      "loss: 0.853781 [57600/60000]\n",
      "loss: 0.043910 [58200/60000]\n",
      "loss: 0.225302 [58800/60000]\n",
      "loss: 0.169324 [59400/60000]\n",
      "Test Error: \n",
      " Accuracy: 87.2%, Avg loss: 0.465059 \n",
      "\n",
      "Epoch 13\n",
      "--------------------------------\n",
      "loss: 0.289165 [    0/60000]\n",
      "loss: 0.020124 [  600/60000]\n",
      "loss: 0.532095 [ 1200/60000]\n",
      "loss: 0.106711 [ 1800/60000]\n",
      "loss: 0.839938 [ 2400/60000]\n",
      "loss: 0.669194 [ 3000/60000]\n",
      "loss: 0.023492 [ 3600/60000]\n",
      "loss: 0.176817 [ 4200/60000]\n",
      "loss: 0.023764 [ 4800/60000]\n",
      "loss: 0.231408 [ 5400/60000]\n",
      "loss: 0.643380 [ 6000/60000]\n",
      "loss: 0.077821 [ 6600/60000]\n",
      "loss: 0.849244 [ 7200/60000]\n",
      "loss: 0.337924 [ 7800/60000]\n",
      "loss: 0.006971 [ 8400/60000]\n",
      "loss: 0.042739 [ 9000/60000]\n",
      "loss: 0.570269 [ 9600/60000]\n",
      "loss: 0.026225 [10200/60000]\n",
      "loss: 0.314213 [10800/60000]\n",
      "loss: 0.357990 [11400/60000]\n",
      "loss: 0.517806 [12000/60000]\n",
      "loss: 0.154047 [12600/60000]\n",
      "loss: 0.431550 [13200/60000]\n",
      "loss: 0.033927 [13800/60000]\n",
      "loss: 0.199775 [14400/60000]\n",
      "loss: 0.031723 [15000/60000]\n",
      "loss: 0.066785 [15600/60000]\n",
      "loss: 0.018771 [16200/60000]\n",
      "loss: 0.743247 [16800/60000]\n",
      "loss: 0.056066 [17400/60000]\n",
      "loss: 0.292083 [18000/60000]\n",
      "loss: 0.002609 [18600/60000]\n",
      "loss: 0.024543 [19200/60000]\n",
      "loss: 0.552554 [19800/60000]\n",
      "loss: 0.296890 [20400/60000]\n",
      "loss: 0.004679 [21000/60000]\n",
      "loss: 1.047243 [21600/60000]\n",
      "loss: 0.002700 [22200/60000]\n",
      "loss: 0.033498 [22800/60000]\n",
      "loss: 0.067418 [23400/60000]\n",
      "loss: 0.474247 [24000/60000]\n",
      "loss: 0.152237 [24600/60000]\n",
      "loss: 0.034255 [25200/60000]\n",
      "loss: 0.575264 [25800/60000]\n",
      "loss: 0.029485 [26400/60000]\n",
      "loss: 0.002145 [27000/60000]\n",
      "loss: 0.148727 [27600/60000]\n",
      "loss: 0.038198 [28200/60000]\n",
      "loss: 0.066017 [28800/60000]\n",
      "loss: 0.000862 [29400/60000]\n",
      "loss: 0.030343 [30000/60000]\n",
      "loss: 0.020095 [30600/60000]\n",
      "loss: 0.820890 [31200/60000]\n",
      "loss: 0.012977 [31800/60000]\n",
      "loss: 0.413966 [32400/60000]\n",
      "loss: 0.511139 [33000/60000]\n",
      "loss: 0.067648 [33600/60000]\n",
      "loss: 0.236652 [34200/60000]\n",
      "loss: 0.940614 [34800/60000]\n",
      "loss: 0.089080 [35400/60000]\n",
      "loss: 0.025518 [36000/60000]\n",
      "loss: 0.017707 [36600/60000]\n",
      "loss: 0.009865 [37200/60000]\n",
      "loss: 0.119444 [37800/60000]\n",
      "loss: 0.002609 [38400/60000]\n",
      "loss: 0.312783 [39000/60000]\n",
      "loss: 0.045109 [39600/60000]\n",
      "loss: 0.144495 [40200/60000]\n",
      "loss: 0.144701 [40800/60000]\n",
      "loss: 0.279351 [41400/60000]\n",
      "loss: 0.119185 [42000/60000]\n",
      "loss: 0.874410 [42600/60000]\n",
      "loss: 0.000201 [43200/60000]\n",
      "loss: 0.116728 [43800/60000]\n",
      "loss: 0.210087 [44400/60000]\n",
      "loss: 0.273467 [45000/60000]\n",
      "loss: 0.119877 [45600/60000]\n",
      "loss: 0.086321 [46200/60000]\n",
      "loss: 0.183279 [46800/60000]\n",
      "loss: 0.159880 [47400/60000]\n",
      "loss: 0.039230 [48000/60000]\n",
      "loss: 0.000321 [48600/60000]\n",
      "loss: 0.035550 [49200/60000]\n",
      "loss: 0.017894 [49800/60000]\n",
      "loss: 0.355656 [50400/60000]\n",
      "loss: 0.238095 [51000/60000]\n",
      "loss: 0.744351 [51600/60000]\n",
      "loss: 0.297288 [52200/60000]\n",
      "loss: 0.125511 [52800/60000]\n",
      "loss: 0.246552 [53400/60000]\n",
      "loss: 0.224193 [54000/60000]\n",
      "loss: 0.046903 [54600/60000]\n",
      "loss: 0.037193 [55200/60000]\n",
      "loss: 0.348803 [55800/60000]\n",
      "loss: 0.210370 [56400/60000]\n",
      "loss: 0.425080 [57000/60000]\n",
      "loss: 0.466138 [57600/60000]\n",
      "loss: 0.589549 [58200/60000]\n",
      "loss: 0.354891 [58800/60000]\n",
      "loss: 0.232803 [59400/60000]\n",
      "Test Error: \n",
      " Accuracy: 87.2%, Avg loss: 0.466753 \n",
      "\n",
      "Epoch 14\n",
      "--------------------------------\n",
      "loss: 0.303625 [    0/60000]\n",
      "loss: 0.072569 [  600/60000]\n",
      "loss: 0.545877 [ 1200/60000]\n",
      "loss: 0.025973 [ 1800/60000]\n",
      "loss: 0.380714 [ 2400/60000]\n",
      "loss: 0.172172 [ 3000/60000]\n",
      "loss: 0.019020 [ 3600/60000]\n",
      "loss: 0.173467 [ 4200/60000]\n",
      "loss: 0.019933 [ 4800/60000]\n",
      "loss: 0.375120 [ 5400/60000]\n",
      "loss: 0.733207 [ 6000/60000]\n",
      "loss: 0.103087 [ 6600/60000]\n",
      "loss: 0.963410 [ 7200/60000]\n",
      "loss: 0.230510 [ 7800/60000]\n",
      "loss: 0.017717 [ 8400/60000]\n",
      "loss: 0.068141 [ 9000/60000]\n",
      "loss: 0.579782 [ 9600/60000]\n",
      "loss: 0.030960 [10200/60000]\n",
      "loss: 0.275159 [10800/60000]\n",
      "loss: 0.471338 [11400/60000]\n",
      "loss: 0.225131 [12000/60000]\n",
      "loss: 0.185730 [12600/60000]\n",
      "loss: 0.330618 [13200/60000]\n",
      "loss: 0.068262 [13800/60000]\n",
      "loss: 0.044463 [14400/60000]\n",
      "loss: 0.022522 [15000/60000]\n",
      "loss: 0.037129 [15600/60000]\n",
      "loss: 0.031337 [16200/60000]\n",
      "loss: 0.776506 [16800/60000]\n",
      "loss: 0.021484 [17400/60000]\n",
      "loss: 0.449619 [18000/60000]\n",
      "loss: 0.032500 [18600/60000]\n",
      "loss: 0.032547 [19200/60000]\n",
      "loss: 0.324118 [19800/60000]\n",
      "loss: 0.191820 [20400/60000]\n",
      "loss: 0.051236 [21000/60000]\n",
      "loss: 0.711106 [21600/60000]\n",
      "loss: 0.003205 [22200/60000]\n",
      "loss: 0.049382 [22800/60000]\n",
      "loss: 0.133791 [23400/60000]\n",
      "loss: 0.520993 [24000/60000]\n",
      "loss: 0.084926 [24600/60000]\n",
      "loss: 0.039613 [25200/60000]\n",
      "loss: 0.420027 [25800/60000]\n",
      "loss: 0.020544 [26400/60000]\n",
      "loss: 0.000233 [27000/60000]\n",
      "loss: 0.128911 [27600/60000]\n",
      "loss: 0.043430 [28200/60000]\n",
      "loss: 0.280548 [28800/60000]\n",
      "loss: 0.000458 [29400/60000]\n",
      "loss: 0.038879 [30000/60000]\n",
      "loss: 0.028009 [30600/60000]\n",
      "loss: 0.577911 [31200/60000]\n",
      "loss: 0.009226 [31800/60000]\n",
      "loss: 0.383889 [32400/60000]\n",
      "loss: 0.386490 [33000/60000]\n",
      "loss: 0.035500 [33600/60000]\n",
      "loss: 0.320279 [34200/60000]\n",
      "loss: 0.897025 [34800/60000]\n",
      "loss: 0.105340 [35400/60000]\n",
      "loss: 0.194215 [36000/60000]\n",
      "loss: 0.029440 [36600/60000]\n",
      "loss: 0.004740 [37200/60000]\n",
      "loss: 0.094196 [37800/60000]\n",
      "loss: 0.000031 [38400/60000]\n",
      "loss: 0.442722 [39000/60000]\n",
      "loss: 0.395995 [39600/60000]\n",
      "loss: 0.285567 [40200/60000]\n",
      "loss: 0.124094 [40800/60000]\n",
      "loss: 0.267311 [41400/60000]\n",
      "loss: 0.010232 [42000/60000]\n",
      "loss: 0.648799 [42600/60000]\n",
      "loss: 0.000394 [43200/60000]\n",
      "loss: 0.151877 [43800/60000]\n",
      "loss: 0.235585 [44400/60000]\n",
      "loss: 0.468835 [45000/60000]\n",
      "loss: 0.160256 [45600/60000]\n",
      "loss: 0.228801 [46200/60000]\n",
      "loss: 0.106946 [46800/60000]\n",
      "loss: 0.220018 [47400/60000]\n",
      "loss: 0.026759 [48000/60000]\n",
      "loss: 0.000154 [48600/60000]\n",
      "loss: 0.007729 [49200/60000]\n",
      "loss: 0.004724 [49800/60000]\n",
      "loss: 0.388836 [50400/60000]\n",
      "loss: 0.201164 [51000/60000]\n",
      "loss: 1.153259 [51600/60000]\n",
      "loss: 0.278534 [52200/60000]\n",
      "loss: 0.149633 [52800/60000]\n",
      "loss: 0.283179 [53400/60000]\n",
      "loss: 0.053778 [54000/60000]\n",
      "loss: 0.079959 [54600/60000]\n",
      "loss: 0.033209 [55200/60000]\n",
      "loss: 0.446761 [55800/60000]\n",
      "loss: 0.195100 [56400/60000]\n",
      "loss: 0.415334 [57000/60000]\n",
      "loss: 0.842088 [57600/60000]\n",
      "loss: 0.367099 [58200/60000]\n",
      "loss: 0.350979 [58800/60000]\n",
      "loss: 0.185248 [59400/60000]\n",
      "Test Error: \n",
      " Accuracy: 87.6%, Avg loss: 0.498773 \n",
      "\n",
      "Epoch 15\n",
      "--------------------------------\n",
      "loss: 0.383457 [    0/60000]\n",
      "loss: 0.007475 [  600/60000]\n",
      "loss: 1.056176 [ 1200/60000]\n",
      "loss: 0.076419 [ 1800/60000]\n",
      "loss: 0.598145 [ 2400/60000]\n",
      "loss: 0.498118 [ 3000/60000]\n",
      "loss: 0.069299 [ 3600/60000]\n",
      "loss: 0.468593 [ 4200/60000]\n",
      "loss: 1.818355 [ 4800/60000]\n",
      "loss: 0.373973 [ 5400/60000]\n",
      "loss: 0.712508 [ 6000/60000]\n",
      "loss: 0.115648 [ 6600/60000]\n",
      "loss: 0.576723 [ 7200/60000]\n",
      "loss: 0.421547 [ 7800/60000]\n",
      "loss: 0.011219 [ 8400/60000]\n",
      "loss: 0.046642 [ 9000/60000]\n",
      "loss: 0.571927 [ 9600/60000]\n",
      "loss: 0.021673 [10200/60000]\n",
      "loss: 0.208740 [10800/60000]\n",
      "loss: 0.366479 [11400/60000]\n",
      "loss: 0.337046 [12000/60000]\n",
      "loss: 0.294095 [12600/60000]\n",
      "loss: 0.379554 [13200/60000]\n",
      "loss: 0.072081 [13800/60000]\n",
      "loss: 0.096680 [14400/60000]\n",
      "loss: 0.038121 [15000/60000]\n",
      "loss: 0.046463 [15600/60000]\n",
      "loss: 0.118243 [16200/60000]\n",
      "loss: 0.834195 [16800/60000]\n",
      "loss: 0.029831 [17400/60000]\n",
      "loss: 0.348692 [18000/60000]\n",
      "loss: 0.013585 [18600/60000]\n",
      "loss: 0.009376 [19200/60000]\n",
      "loss: 0.301083 [19800/60000]\n",
      "loss: 0.074663 [20400/60000]\n",
      "loss: 0.002426 [21000/60000]\n",
      "loss: 0.739444 [21600/60000]\n",
      "loss: 0.000498 [22200/60000]\n",
      "loss: 0.135312 [22800/60000]\n",
      "loss: 0.140863 [23400/60000]\n",
      "loss: 0.528265 [24000/60000]\n",
      "loss: 0.167157 [24600/60000]\n",
      "loss: 0.053011 [25200/60000]\n",
      "loss: 0.569324 [25800/60000]\n",
      "loss: 0.000399 [26400/60000]\n",
      "loss: 0.000027 [27000/60000]\n",
      "loss: 0.052507 [27600/60000]\n",
      "loss: 0.036975 [28200/60000]\n",
      "loss: 0.061475 [28800/60000]\n",
      "loss: 0.000427 [29400/60000]\n",
      "loss: 0.062654 [30000/60000]\n",
      "loss: 0.014730 [30600/60000]\n",
      "loss: 0.581656 [31200/60000]\n",
      "loss: 0.018906 [31800/60000]\n",
      "loss: 0.540941 [32400/60000]\n",
      "loss: 0.357692 [33000/60000]\n",
      "loss: 0.022356 [33600/60000]\n",
      "loss: 0.246572 [34200/60000]\n",
      "loss: 1.049739 [34800/60000]\n",
      "loss: 0.082991 [35400/60000]\n",
      "loss: 0.073068 [36000/60000]\n",
      "loss: 0.017566 [36600/60000]\n",
      "loss: 0.000731 [37200/60000]\n",
      "loss: 0.138590 [37800/60000]\n",
      "loss: 0.001078 [38400/60000]\n",
      "loss: 0.263064 [39000/60000]\n",
      "loss: 0.196686 [39600/60000]\n",
      "loss: 0.206330 [40200/60000]\n",
      "loss: 0.171305 [40800/60000]\n",
      "loss: 0.360059 [41400/60000]\n",
      "loss: 0.017164 [42000/60000]\n",
      "loss: 0.558524 [42600/60000]\n",
      "loss: 0.000011 [43200/60000]\n",
      "loss: 0.418201 [43800/60000]\n",
      "loss: 0.150490 [44400/60000]\n",
      "loss: 0.314282 [45000/60000]\n",
      "loss: 0.185560 [45600/60000]\n",
      "loss: 0.083096 [46200/60000]\n",
      "loss: 0.186251 [46800/60000]\n",
      "loss: 0.191067 [47400/60000]\n",
      "loss: 0.018930 [48000/60000]\n",
      "loss: 0.016429 [48600/60000]\n",
      "loss: 0.013396 [49200/60000]\n",
      "loss: 0.018063 [49800/60000]\n",
      "loss: 0.190121 [50400/60000]\n",
      "loss: 0.128738 [51000/60000]\n",
      "loss: 0.816088 [51600/60000]\n",
      "loss: 0.329438 [52200/60000]\n",
      "loss: 0.091378 [52800/60000]\n",
      "loss: 0.233780 [53400/60000]\n",
      "loss: 0.051338 [54000/60000]\n",
      "loss: 0.023115 [54600/60000]\n",
      "loss: 0.193459 [55200/60000]\n",
      "loss: 0.337718 [55800/60000]\n",
      "loss: 0.169777 [56400/60000]\n",
      "loss: 0.295608 [57000/60000]\n",
      "loss: 1.023226 [57600/60000]\n",
      "loss: 0.009269 [58200/60000]\n",
      "loss: 0.199723 [58800/60000]\n",
      "loss: 0.176500 [59400/60000]\n",
      "Test Error: \n",
      " Accuracy: 87.2%, Avg loss: 0.502925 \n",
      "\n",
      "Done training\n"
     ]
    }
   ],
   "source": [
    "epochs = 15\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n--------------------------------\")\n",
    "    train(train_loader, model, loss_fn, optimizer)\n",
    "    test(test_loader, model, loss_fn)\n",
    "print(\"Done training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), \"model.pth\")\n",
    "print(\"Model saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5234\n",
      "Predicted: \"Sneaker\", Actual: Sneaker\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x232356ac6a0>"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork()\n",
    "model.load_state_dict(torch.load(\"model.pth\"))\n",
    "\n",
    "classes = [\n",
    "    \"T-shirt/top\",\n",
    "    \"Trouser\",\n",
    "    \"Pullover\",\n",
    "    \"Dress\",\n",
    "    \"Coat\",\n",
    "    \"Sandal\",\n",
    "    \"Shirt\",\n",
    "    \"Sneaker\",\n",
    "    \"Bag\",\n",
    "    \"Ankle boot\"\n",
    "]\n",
    "\n",
    "# pic random number\n",
    "import random\n",
    "rand_sample = int(random.random() * test_data.__len__())\n",
    "print(rand_sample)\n",
    "\n",
    "model.eval()\n",
    "x, y = test_data[rand_sample][0], test_data[rand_sample][1]\n",
    "with torch.no_grad():\n",
    "    pred = model(x)\n",
    "    predicted, actual = classes[pred[0].argmax(0)], classes[y]\n",
    "    print(f'Predicted: \"{predicted}\", Actual: {actual}')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "#show image\n",
    "plt.imshow(x.T.rot90().rot90().rot90(), cmap='gray')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7b8c902a5db4bd22b43399a271754202218dc4d05c7622b2f621f708dd6b6e04"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
