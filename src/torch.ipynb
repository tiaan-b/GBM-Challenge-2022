{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import polars as pl\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "import tqdm\n",
    "\n",
    "import helpers.input_processor as ip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data from save file:  cache/ingested_data.json\n"
     ]
    }
   ],
   "source": [
    "#load data from dataframe\n",
    "data_dir = \"data/raw_training/training_data/\"\n",
    "target_label = 'murmur_in_recording'\n",
    "df = (\n",
    "    ip.loadTrainingData(data_dir)\n",
    "    .filter(pl.col(target_label) != 'Unknown')\n",
    "    .pipe(ip.encodeData)\n",
    "    .select([\n",
    "        pl.col('audio_file').apply(lambda x: os.path.join(data_dir, x)),\n",
    "        pl.col(target_label)\n",
    "    ])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Samples:        5328\n",
      "Positive Samples:     2664\n",
      "Negative Samples:     2664\n",
      "Percent Positive Samples:     0.5\n"
     ]
    }
   ],
   "source": [
    "#balance the data so that there is an equal number of murmur positive and murmur negative samples\n",
    "#do this by duplicating random rows of whichever group (pos or neg) is smaller\n",
    "neg_df = df.filter(pl.col(target_label)==0.0)\n",
    "pos_df = df.filter(pl.col(target_label)==1.0)\n",
    "numNeg = neg_df.height\n",
    "numPos = pos_df.height\n",
    "\n",
    "while numNeg != numPos:\n",
    "    if numNeg < numPos:\n",
    "        df.vstack(neg_df.sample(n=min(numPos-numNeg, neg_df.height), shuffle=True), in_place=True)\n",
    "    else: \n",
    "        df.vstack(pos_df.sample(n=min(numNeg-numPos, pos_df.height), shuffle=True), in_place=True)\n",
    "    numNeg = df.filter(pl.col(target_label)==0.0).height\n",
    "    numPos = df.filter(pl.col(target_label)==1.0).height\n",
    "\n",
    "#reshuffle rows\n",
    "df = df.sample(frac=1.0, shuffle=True)\n",
    "\n",
    "#check number of positive and negative samples\n",
    "numNeg = df.filter(pl.col(target_label)==0.0).height\n",
    "numPos = df.filter(pl.col(target_label)==1.0).height\n",
    "print('Total Samples:       ', df.height)\n",
    "print('Positive Samples:    ', numPos)\n",
    "print('Negative Samples:    ', numNeg)\n",
    "print('Percent Positive Samples:    ', numPos/(numPos+numNeg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import math, random\n",
    "import numpy as np\n",
    "\n",
    "# x = path to audio file\n",
    "# duration = length of time (in seconds) to which the signal is resized\n",
    "# sr = sample rate of the signal\n",
    "# **kwargs = keyword args to pass to librosa.feature.melspectrogram()\n",
    "def preprocessAudio(x, duration, sr=4000, **kwargs):\n",
    "    #read and load audio file in .wav format\n",
    "    sig, samp_rate = librosa.load(x, sr=sr) #make sure that the correct sample rate is passed as a parameters. if unspecified, the function chooses some default value\n",
    "    \n",
    "    #resize sample, either by padding it with silence or truncating it\n",
    "    sig = librosa.util.fix_length(sig, size=sr*duration)\n",
    "\n",
    "    #implement audio augmentation:  ------------\n",
    "    #time shift signal to the left or right by a random percent of its original length (max 99%)\n",
    "    sig_len = sig.shape[0]\n",
    "    max_shift = 0.99\n",
    "    sig = np.roll(sig, round(random.random() * max_shift * sig_len))\n",
    "\n",
    "    # #convert to stereo\n",
    "    # sig = np.tile(sig, (2,1))\n",
    "    #-------------------------------------------\n",
    "\n",
    "    #get Mel spectrogram\n",
    "    melSpec = librosa.feature.melspectrogram(y=sig, sr=samp_rate)\n",
    "    melSpec = librosa.amplitude_to_db(melSpec)\n",
    "\n",
    "    #implement image augmentation   ------------\n",
    "    #-------------------------------------------\n",
    "\n",
    "    return melSpec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, audioPaths, labels):\n",
    "        self.audioPaths = audioPaths\n",
    "        self.labels = labels\n",
    "        self.sample_duration = 25\n",
    "        self.n_mels = 128\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        spec = torch.from_numpy(preprocessAudio(self.audioPaths[idx], self.sample_duration, n_mels=self.n_mels))\n",
    "        label = self.labels[idx]\n",
    "        return spec, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitDataset(ds, split_ratio=0.8):\n",
    "    total_size = len(ds)\n",
    "    train_size = round(split_ratio * total_size)\n",
    "    test_size = total_size - train_size\n",
    "\n",
    "    trainSet, testSet = torch.utils.data.random_split(ds, [train_size, test_size], generator=torch.Generator().manual_seed(0))\n",
    "    return trainSet, testSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "audioPaths = df.get_column('audio_file').to_list()\n",
    "labels = df.get_column(target_label).to_list()\n",
    "classes = df.get_column(target_label).unique().to_list()\n",
    "\n",
    "ds = AudioDataset(audioPaths, labels)\n",
    "train_ds, test_ds = splitDataset(ds)\n",
    "\n",
    "# train_dl = torch.utils.data.DataLoader(train_ds, batch_size=16, shuffle=True)\n",
    "# val_dl = torch.utils.data.DataLoader(val_ds, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________________________________________________________________________________________________________\n",
    "tutorial code\n",
    "_________________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyper-parameters \n",
    "num_epochs = 5\n",
    "batch_size = 32\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Dataloaders\n",
    "train_loader = torch.utils.data.DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_ds, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        # self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        # self.pool = nn.MaxPool2d(2, 2)\n",
    "        # self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        # self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        # self.fc2 = nn.Linear(120, 84)\n",
    "        # self.fc3 = nn.Linear(84, 10)\n",
    "        numClasses = 2\n",
    "        self.conv1 = nn.Conv2d(1, 2*1, 5)   # first param = 1 since iput image has 1 channel\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(2*1, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 29 * 46, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, numClasses)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # -> n, 3, 32, 32\n",
    "        x = self.pool(F.relu(self.conv1(x)))  # -> n, 6, 14, 14\n",
    "        x = self.pool(F.relu(self.conv2(x)))  # -> n, 16, 5, 5\n",
    "        x = x.view(-1, 16 * 29 * 46)            # -> n, 400\n",
    "        x = F.relu(self.fc1(x))               # -> n, 120\n",
    "        x = F.relu(self.fc2(x))               # -> n, 84\n",
    "        x = self.fc3(x)                       # -> n, 10\n",
    "        return x\n",
    "\n",
    "\n",
    "model = ConvNet().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 134/134 [01:05<00:00,  2.03it/s, loss=0.562]\n",
      "Epoch 1: 100%|██████████| 134/134 [01:08<00:00,  1.96it/s, loss=0.628]\n",
      "Epoch 2: 100%|██████████| 134/134 [01:05<00:00,  2.05it/s, loss=0.629]\n",
      "Epoch 3: 100%|██████████| 134/134 [01:01<00:00,  2.17it/s, loss=0.691]\n",
      "Epoch 4: 100%|██████████| 134/134 [01:02<00:00,  2.16it/s, loss=0.626]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "n_total_steps = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    epoch_bar = tqdm.tqdm(train_loader)\n",
    "    epoch_bar.set_description(\"Epoch %s\" % epoch)\n",
    "    for i, (images, labels) in enumerate(epoch_bar):\n",
    "        epoch_bar.set_postfix({\"loss\": running_loss})\n",
    "\n",
    "        # origin shape: [4, 3, 32, 32] = 4, 3, 1024\n",
    "        # input_layer: 3 input channels, 6 output channels, 5 kernel size\n",
    "        images = images.to(device)\n",
    "        labels = labels.type(torch.LongTensor).to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images.unsqueeze(1))\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss = loss.item()\n",
    "\n",
    "print('Finished Training')\n",
    "PATH = './cnn.pth'\n",
    "torch.save(model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 10 is out of bounds for dimension 0 with size 10",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/tiaan/GBM-Challenge-2022/src/torch.ipynb Cell 11\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2252632d677075312e6f6b2e5562632e6361227d/home/tiaan/GBM-Challenge-2022/src/torch.ipynb#ch0000010vscode-remote?line=12'>13</a>\u001b[0m n_correct \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (predicted \u001b[39m==\u001b[39m labels)\u001b[39m.\u001b[39msum()\u001b[39m.\u001b[39mitem()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2252632d677075312e6f6b2e5562632e6361227d/home/tiaan/GBM-Challenge-2022/src/torch.ipynb#ch0000010vscode-remote?line=14'>15</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(batch_size):\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2252632d677075312e6f6b2e5562632e6361227d/home/tiaan/GBM-Challenge-2022/src/torch.ipynb#ch0000010vscode-remote?line=15'>16</a>\u001b[0m     label \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(labels[i]\u001b[39m.\u001b[39mitem())\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2252632d677075312e6f6b2e5562632e6361227d/home/tiaan/GBM-Challenge-2022/src/torch.ipynb#ch0000010vscode-remote?line=16'>17</a>\u001b[0m     pred \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(predicted[i]\u001b[39m.\u001b[39mitem())\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2252632d677075312e6f6b2e5562632e6361227d/home/tiaan/GBM-Challenge-2022/src/torch.ipynb#ch0000010vscode-remote?line=17'>18</a>\u001b[0m     \u001b[39mif\u001b[39;00m (label \u001b[39m==\u001b[39m pred):\n",
      "\u001b[0;31mIndexError\u001b[0m: index 10 is out of bounds for dimension 0 with size 10"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    n_correct = 0\n",
    "    n_samples = 0\n",
    "    n_class_correct = [0 for i in range(2)]\n",
    "    n_class_samples = [0 for i in range(2)]\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images.unsqueeze(1))\n",
    "        # max returns (value ,index)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        n_samples += labels.size(0)\n",
    "        n_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            label = int(labels[i].item())\n",
    "            pred = int(predicted[i].item())\n",
    "            if (label == pred):\n",
    "                n_class_correct[label] += 1\n",
    "            n_class_samples[label] += 1\n",
    "\n",
    "    acc = 100.0 * n_correct / n_samples\n",
    "    print(f'Accuracy of the network: {acc} %')\n",
    "\n",
    "    for i in range(10):\n",
    "        acc = 100.0 * n_class_correct[i] / n_class_samples[i]\n",
    "        print(f'Accuracy of {classes[i]}: {acc} %')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
